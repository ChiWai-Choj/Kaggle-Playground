{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline, grid_search\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import re\n",
    "import random\n",
    "random.seed(2016)\n",
    "\n",
    "stop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing'] #'electr','paint','pipe','light','kitchen','wood','outdoor','door','bathroom'\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str_stem(s): \n",
    "    if isinstance(s, str):\n",
    "        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n",
    "        s = s.lower()\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = s.replace(\",\",\"\") #could be number / segment later\n",
    "        s = s.replace(\"$\",\" \")\n",
    "        s = s.replace(\"?\",\" \")\n",
    "        s = s.replace(\"-\",\" \")\n",
    "        s = s.replace(\"//\",\"/\")\n",
    "        s = s.replace(\"..\",\".\")\n",
    "        s = s.replace(\" / \",\" \")\n",
    "        s = s.replace(\" \\\\ \",\" \")\n",
    "        s = s.replace(\".\",\" . \")\n",
    "        s = re.sub(r\"(^\\.|/)\", r\"\", s)\n",
    "        s = re.sub(r\"(\\.|/)$\", r\"\", s)\n",
    "        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "        s = s.replace(\" x \",\" xbi \")\n",
    "        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "        s = s.replace(\"*\",\" xbi \")\n",
    "        s = s.replace(\" by \",\" xbi \")\n",
    "        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "        s = s.replace(\"Â°\",\" degrees \")\n",
    "        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "        s = s.replace(\" v \",\" volts \")\n",
    "        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = s.replace(\" . \",\" \")\n",
    "        #s = (\" \").join([z for z in s.split(\" \") if z not in stop_w])\n",
    "        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n",
    "        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "        \n",
    "        s = s.lower()\n",
    "        s = s.replace(\"toliet\",\"toilet\")\n",
    "        s = s.replace(\"airconditioner\",\"air conditioner\")\n",
    "        s = s.replace(\"vinal\",\"vinyl\")\n",
    "        s = s.replace(\"vynal\",\"vinyl\")\n",
    "        s = s.replace(\"skill\",\"skil\")\n",
    "        s = s.replace(\"snowbl\",\"snow bl\")\n",
    "        s = s.replace(\"plexigla\",\"plexi gla\")\n",
    "        s = s.replace(\"rustoleum\",\"rust-oleum\")\n",
    "        s = s.replace(\"whirpool\",\"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolga\", \"whirlpool ga\")\n",
    "        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n",
    "        return s\n",
    "    else:\n",
    "        return \"null\"\n",
    "\n",
    "def seg_words(str1, str2):\n",
    "    str2 = str2.lower()\n",
    "    str2 = re.sub(\"[^a-z0-9./]\",\" \", str2)\n",
    "    str2 = [z for z in set(str2.split()) if len(z)>2]\n",
    "    words = str1.lower().split(\" \")\n",
    "    s = []\n",
    "    for word in words:\n",
    "        if len(word)>3:\n",
    "            s1 = []\n",
    "            s1 += segmentit(word,str2,True)\n",
    "            if len(s)>1:\n",
    "                s += [z for z in s1 if z not in ['er','ing','s','less'] and len(z)>1]\n",
    "            else:\n",
    "                s.append(word)\n",
    "        else:\n",
    "            s.append(word)\n",
    "    return (\" \".join(s))\n",
    "\n",
    "def segmentit(s, txt_arr, t):\n",
    "    st = s\n",
    "    r = []\n",
    "    for j in range(len(s)):\n",
    "        for word in txt_arr:\n",
    "            if word == s[:-j]:\n",
    "                r.append(s[:-j])\n",
    "                #print(s[:-j],s[len(s)-j:])\n",
    "                s=s[len(s)-j:]\n",
    "                r += segmentit(s, txt_arr, False)\n",
    "    if t:\n",
    "        i = len((\"\").join(r))\n",
    "        if not i==len(st):\n",
    "            r.append(st[i:])\n",
    "    return r\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "      return data_dict[self.key].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files Loaded: 1.85 minutes ---\n",
      "--- Stemming: 13.49 minutes ---\n",
      "--- Prod Info: 13.49 minutes ---\n",
      "--- Len of: 13.56 minutes ---\n",
      "--- Query In: 13.77 minutes ---\n",
      "--- Query Last Word In: 13.82 minutes ---\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./features/train.csv', encoding=\"ISO-8859-1\") #update here\n",
    "df_test = pd.read_csv('./features/test.csv', encoding=\"ISO-8859-1\") #update here\n",
    "df_pro_desc = pd.read_csv('./features/product_descriptions.csv') #update here\n",
    "df_attr = pd.read_csv('./features/attributes.csv')\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "num_train = df_train.shape[0]\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "#comment out the lines below use df_all.csv for further grid search testing\n",
    "#if adding features consider any drops on the 'cust_regression_vals' class\n",
    "#*** would be nice to have a file reuse option or script chaining option on Kaggle Scripts ***\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stem(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stem(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stem(x))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x:str_stem(x))\n",
    "print(\"--- Stemming: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title'] +\"\\t\"+df_all['product_description']\n",
    "print(\"--- Prod Info: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "print(\"--- Len of: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['search_term'] = df_all['product_info'].map(lambda x:seg_words(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "#print(\"--- Search Term Segment: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "df_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "print(\"--- Query In: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['query_last_word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[1]))\n",
    "df_all['query_last_word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[2]))\n",
    "print(\"--- Query Last Word In: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "df_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\n",
    "df_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\n",
    "df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1000\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=3\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x:d[x])\n",
    "df_all['search_term_feature'] = df_all['search_term'].map(lambda x:len(x))\n",
    "df_all.to_csv('./features/df_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
